import streamlit as st

# Import du wrapper LLM et des helpers
import source
from source.llm_client import OllamaClient
from source.prompts import build_messages
from source.config import OLLAMA_MODEL


# Configuration de la page
st.set_page_config(page_title="ESILV Smart Assistant", page_icon="icon.png")

# Titre principal
st.title("ü§ñ ESILV Smart Assistant")
st.write("Prototype minimal : un message ‚Üí une r√©ponse (pas encore de m√©moire).")

# Initialisation du client LLM
client = OllamaClient(model=OLLAMA_MODEL)

# Zone de chat Streamlit
user_input = st.chat_input("√âcris ton message ici...")

if user_input:
    # Afficher le message de l'utilisateur
    with st.chat_message("user"):
        st.write(user_input)

    # Construire les messages pour le mod√®le
    messages = build_messages(user_input)

    # Appel au mod√®le
    with st.chat_message("assistant"):
        with st.spinner("Le mod√®le r√©fl√©chit..."):
            try:
                response = client.generate(messages)
                st.write(response)
            except Exception as e:
                st.error(f"Erreur lors de l'appel au mod√®le : {e}")
